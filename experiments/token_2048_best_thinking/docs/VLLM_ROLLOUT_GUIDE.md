# vLLM Rollout è„šæœ¬ä½¿ç”¨æŒ‡å—

è½»é‡åŒ–çš„ vLLM Rollout è„šæœ¬ï¼Œç”¨äºé«˜æ•ˆçš„æ‰¹é‡æ¨ç†ã€‚

## ç‰¹ç‚¹

- âœ¨ **é«˜æ€§èƒ½**: ä½¿ç”¨ vLLM å¼•æ“ï¼Œç›¸æ¯” transformers æä¾›æ›´é«˜çš„ååé‡
- ğŸš€ **æ˜“ç”¨**: æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ä¸¤ç§æ–¹å¼
- ğŸ¯ **çµæ´»**: æ”¯æŒè‡ªå®šä¹‰é‡‡æ ·å‚æ•°ã€å¤šæ ·æœ¬ç”Ÿæˆ
- ğŸ’¾ **ç®€æ´**: è¾“å…¥è¾“å‡ºéƒ½æ˜¯ parquet æ ¼å¼ï¼Œæ–¹ä¾¿æ•°æ®å¤„ç†
- ğŸ”§ **å¯é…ç½®**: æ”¯æŒå¼ é‡å¹¶è¡Œã€æ˜¾å­˜ä¼˜åŒ–ç­‰é«˜çº§ç‰¹æ€§

## å®‰è£…ä¾èµ–

```bash
# å®‰è£… vLLMï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
pip install vllm

# æˆ–è€…ä»æºç å®‰è£…æœ€æ–°ç‰ˆæœ¬
pip install git+https://github.com/vllm-project/vllm.git
```

## å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°

```bash
python scripts/vllm_rollout.py \
    --model_path /datacenter/models/Qwen/Qwen3-4B-Instruct-2507 \
    --input outputs/stage1_sampled_questions.parquet \
    --output outputs/vllm_rollout_output.parquet \
    --n_samples 8 \
    --max_tokens 2048
```

### æ–¹æ³• 2: ä½¿ç”¨é…ç½®æ–‡ä»¶

```bash
# 1. ç¼–è¾‘é…ç½®æ–‡ä»¶
vim configs/vllm_rollout_config.yaml

# 2. è¿è¡Œè„šæœ¬
python scripts/vllm_rollout_with_config.py \
    --config configs/vllm_rollout_config.yaml
```

### æ–¹æ³• 3: ä½¿ç”¨ Bash è„šæœ¬

```bash
# ä½¿ç”¨é»˜è®¤å‚æ•°
bash scripts/run_vllm_rollout.sh

# æˆ–è€…é€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰å‚æ•°
MODEL_PATH=/path/to/model \
N_SAMPLES=16 \
MAX_TOKENS=4096 \
bash scripts/run_vllm_rollout.sh
```

## è¾“å…¥è¾“å‡ºæ ¼å¼

### è¾“å…¥æ ¼å¼

è¾“å…¥æ–‡ä»¶å¿…é¡»æ˜¯ parquet æ ¼å¼ï¼ŒåŒ…å« `prompt` åˆ—ï¼š

```python
import pandas as pd

# ç¤ºä¾‹ 1: å­—ç¬¦ä¸²æ ¼å¼çš„ prompt
df = pd.DataFrame({
    'q_id': [0, 1, 2],
    'prompt': [
        "What is 2+2?",
        "Explain quantum computing.",
        "Write a poem about AI."
    ]
})

# ç¤ºä¾‹ 2: Chat æ ¼å¼çš„ prompt
df = pd.DataFrame({
    'q_id': [0, 1],
    'prompt': [
        [{"role": "user", "content": "What is 2+2?"}],
        [{"role": "user", "content": "Explain quantum computing."}]
    ]
})

df.to_parquet('input.parquet', index=False)
```

### è¾“å‡ºæ ¼å¼

è¾“å‡ºæ–‡ä»¶åŒ…å«åŸå§‹åˆ— + `responses` åˆ—ï¼š

```python
import pandas as pd

df = pd.read_parquet('output.parquet')

# df åŒ…å«:
# - æ‰€æœ‰åŸå§‹åˆ—ï¼ˆå¦‚ q_id, prompt ç­‰ï¼‰
# - responses: åˆ—è¡¨ï¼ŒåŒ…å« n_samples ä¸ªç”Ÿæˆçš„å“åº”

# ç¤ºä¾‹ï¼šè®¿é—®ç¬¬ä¸€ä¸ªé—®é¢˜çš„æ‰€æœ‰å“åº”
responses_for_q0 = df.iloc[0]['responses']  # é•¿åº¦ä¸º n_samples çš„åˆ—è¡¨
print(f"ç¬¬ 1 ä¸ªå“åº”: {responses_for_q0[0]}")
print(f"ç¬¬ 2 ä¸ªå“åº”: {responses_for_q0[1]}")
```

## é…ç½®å‚æ•°è¯¦è§£

### æ¨¡å‹é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model_path` | str | å¿…éœ€ | æ¨¡å‹è·¯å¾„ |
| `trust_remote_code` | bool | True | æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆQwen ç­‰æ¨¡å‹éœ€è¦ï¼‰ |

### é‡‡æ ·é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `n_samples` | int | 8 | æ¯ä¸ª prompt ç”Ÿæˆå¤šå°‘ä¸ªå“åº” |
| `max_tokens` | int | 2048 | æœ€å¤§ç”Ÿæˆ token æ•° |
| `temperature` | float | 1.0 | é‡‡æ ·æ¸©åº¦ï¼ˆè¶Šä½è¶Šç¡®å®šæ€§ï¼‰ |
| `top_p` | float | 0.95 | Nucleus sampling å‚æ•° |
| `top_k` | int | -1 | Top-k samplingï¼ˆ-1 è¡¨ç¤ºä¸ä½¿ç”¨ï¼‰ |
| `repetition_penalty` | float | 1.0 | é‡å¤æƒ©ç½šï¼ˆ1.0 è¡¨ç¤ºæ— æƒ©ç½šï¼‰ |
| `seed` | int | 42 | éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰ |

### vLLM å¼•æ“é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `tensor_parallel_size` | int | 1 | å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆGPU æ•°é‡ï¼‰ |
| `gpu_memory_utilization` | float | 0.9 | GPU æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆ0-1ï¼‰ |
| `max_model_len` | int | None | æ¨¡å‹æœ€å¤§é•¿åº¦ |
| `dtype` | str | bfloat16 | æ•°æ®ç±»å‹ |
| `enforce_eager` | bool | False | å¼ºåˆ¶ eager æ¨¡å¼ |
| `enable_prefix_caching` | bool | True | å¯ç”¨å‰ç¼€ç¼“å­˜ |
| `max_num_seqs` | int | 256 | æœ€å¤§å¹¶å‘åºåˆ—æ•° |

## æ€§èƒ½è°ƒä¼˜å»ºè®®

### å• GPU (å¦‚ A100 80GB)

```yaml
engine:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.95
  max_num_seqs: 256
  enable_prefix_caching: true
  dtype: bfloat16
```

**é¢„æœŸæ€§èƒ½**: ~2000-3000 tokens/s

### å¤š GPU (å¦‚ 2x A100)

```yaml
engine:
  tensor_parallel_size: 2
  gpu_memory_utilization: 0.95
  max_num_seqs: 512
  enable_prefix_caching: true
  dtype: bfloat16
```

**é¢„æœŸæ€§èƒ½**: ~4000-6000 tokens/s

### æ˜¾å­˜ä¸è¶³æ—¶

```yaml
engine:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.8  # é™ä½æ˜¾å­˜ä½¿ç”¨
  max_num_seqs: 128  # å‡å°‘å¹¶å‘æ•°
  dtype: bfloat16
```

### æé«˜ååé‡

```yaml
engine:
  tensor_parallel_size: 2
  gpu_memory_utilization: 0.95
  max_num_seqs: 512  # å¢åŠ å¹¶å‘æ•°
  enable_prefix_caching: true  # å¯ç”¨ç¼“å­˜
  dtype: bfloat16  # ä½¿ç”¨ bfloat16
```

## å¸¸è§é—®é¢˜

### 1. OOM (æ˜¾å­˜ä¸è¶³)

**é—®é¢˜**: è¿è¡Œæ—¶æŠ¥ CUDA out of memory é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é™ä½æ˜¾å­˜åˆ©ç”¨ç‡
python scripts/vllm_rollout.py \
    --gpu_memory_utilization 0.7 \
    --max_num_seqs 64 \
    ...
```

### 2. é€Ÿåº¦å¤ªæ…¢

**é—®é¢˜**: ç”Ÿæˆé€Ÿåº¦ä¸å¦‚é¢„æœŸ

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¢åŠ å¹¶å‘æ•°å’Œæ˜¾å­˜åˆ©ç”¨ç‡
python scripts/vllm_rollout.py \
    --gpu_memory_utilization 0.95 \
    --max_num_seqs 512 \
    --enable_prefix_caching \
    ...
```

### 3. å¤š GPU ä¸å·¥ä½œ

**é—®é¢˜**: ä½¿ç”¨å¤šä¸ª GPU ä½†é€Ÿåº¦æ²¡æœ‰æå‡

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿è®¾ç½®äº†æ­£ç¡®çš„å¼ é‡å¹¶è¡Œå¤§å°
python scripts/vllm_rollout.py \
    --tensor_parallel_size 2 \  # åº”ç­‰äº GPU æ•°é‡
    ...
```

### 4. ç»“æœä¸å¯å¤ç°

**é—®é¢˜**: å¤šæ¬¡è¿è¡Œç»“æœä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è®¾ç½®å›ºå®šçš„éšæœºç§å­
python scripts/vllm_rollout.py \
    --seed 42 \
    ...
```

## ä¸å…¶ä»–æ–¹æ¡ˆçš„å¯¹æ¯”

### vs. transformers (simple_generate.py)

| ç‰¹æ€§ | vLLM Rollout | Transformers |
|------|--------------|--------------|
| ååé‡ | ğŸš€ é«˜ï¼ˆ2-5xï¼‰ | ä¸€èˆ¬ |
| æ˜¾å­˜æ•ˆç‡ | ğŸ’ª ä¼˜ç§€ | ä¸€èˆ¬ |
| æ˜“ç”¨æ€§ | âœ… ç®€å• | âœ… ç®€å• |
| æ‰¹é‡æ¨ç† | âœ… ä¼˜åŒ– | âŒ æœªä¼˜åŒ– |
| å¯åŠ¨æ—¶é—´ | â±ï¸ è¾ƒæ…¢ | â±ï¸ è¾ƒå¿« |

**æ¨èä½¿ç”¨åœºæ™¯**:
- **vLLM**: å¤§è§„æ¨¡æ‰¹é‡æ¨ç†ï¼ˆ>100 promptsï¼‰
- **transformers**: å°è§„æ¨¡æµ‹è¯•ã€è°ƒè¯•

### vs. verl å®Œæ•´æ¡†æ¶

| ç‰¹æ€§ | vLLM Rollout | verl æ¡†æ¶ |
|------|--------------|-----------|
| åŠŸèƒ½ | ä»…æ¨ç† | å®Œæ•´ RLHF |
| å¤æ‚åº¦ | ğŸŸ¢ ä½ | ğŸ”´ é«˜ |
| ä¾èµ– | å°‘ | å¤š |
| çµæ´»æ€§ | é«˜ | ä¸­ |
| æ€§èƒ½ | ä¼˜ç§€ | ä¼˜ç§€ |

**æ¨èä½¿ç”¨åœºæ™¯**:
- **vLLM Rollout**: åªéœ€è¦ç”Ÿæˆå“åº”ï¼Œä¸éœ€è¦è®­ç»ƒ
- **verl æ¡†æ¶**: éœ€è¦å®Œæ•´çš„ RLHF è®­ç»ƒæµç¨‹

## é«˜çº§ç”¨æ³•

### å¤„ç†å¤§è§„æ¨¡æ•°æ®

å¯¹äºéå¸¸å¤§çš„æ•°æ®é›†ï¼Œå¯ä»¥åˆ†æ‰¹å¤„ç†ï¼š

```python
import pandas as pd

# è¯»å–å¤§æ–‡ä»¶
df = pd.read_parquet('large_input.parquet')

# åˆ†æ‰¹å¤„ç†
batch_size = 1000
for i in range(0, len(df), batch_size):
    batch_df = df.iloc[i:i+batch_size]
    batch_df.to_parquet(f'batch_{i}.parquet', index=False)
    
    # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
    os.system(f'python scripts/vllm_rollout.py \
        --input batch_{i}.parquet \
        --output batch_{i}_output.parquet \
        ...')

# åˆå¹¶ç»“æœ
results = []
for i in range(0, len(df), batch_size):
    results.append(pd.read_parquet(f'batch_{i}_output.parquet'))
final_df = pd.concat(results, ignore_index=True)
final_df.to_parquet('final_output.parquet', index=False)
```

### è‡ªå®šä¹‰ prompt æ ¼å¼

å¦‚æœä½ çš„ prompt åˆ—åä¸æ˜¯ `prompt`ï¼š

```bash
python scripts/vllm_rollout.py \
    --prompt_key "my_custom_prompt_column" \
    ...
```

### ä¸ Ray é›†æˆ

å¯¹äºè¶…å¤§è§„æ¨¡åˆ†å¸ƒå¼æ¨ç†ï¼Œå¯ä»¥ä¸ Ray é›†æˆï¼š

```python
import ray
from vllm_rollout import vllm_rollout

ray.init(address='auto')

@ray.remote
def process_batch(input_path, output_path):
    vllm_rollout(
        model_path=...,
        input_parquet=input_path,
        output_parquet=output_path,
        ...
    )

# å¹¶è¡Œå¤„ç†å¤šä¸ªæ‰¹æ¬¡
futures = [
    process_batch.remote(f'batch_{i}.parquet', f'output_{i}.parquet')
    for i in range(num_batches)
]
ray.get(futures)
```

## ç¤ºä¾‹ï¼šå®Œæ•´çš„å·¥ä½œæµ

```bash
#!/bin/bash
# å®Œæ•´çš„ rollout å·¥ä½œæµç¤ºä¾‹

# 1. å‡†å¤‡æ•°æ®
python scripts/prepare_data.py \
    --output outputs/questions.parquet

# 2. æ‰§è¡Œ rollout
python scripts/vllm_rollout.py \
    --model_path /datacenter/models/Qwen/Qwen3-4B-Instruct-2507 \
    --input outputs/questions.parquet \
    --output outputs/responses.parquet \
    --n_samples 16 \
    --max_tokens 2048 \
    --temperature 1.0 \
    --top_p 0.95 \
    --tensor_parallel_size 2 \
    --gpu_memory_utilization 0.95

# 3. åå¤„ç†
python scripts/post_process.py \
    --input outputs/responses.parquet \
    --output outputs/final_results.parquet

echo "å®Œæˆï¼"
```

## ç›‘æ§å’Œè°ƒè¯•

### æŸ¥çœ‹ vLLM æ—¥å¿—

vLLM ä¼šè¾“å‡ºè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- ååé‡ (tokens/s)
- GPU åˆ©ç”¨ç‡
- ç¼“å­˜å‘½ä¸­ç‡

### æ€§èƒ½åˆ†æ

```bash
# ä½¿ç”¨ nvidia-smi ç›‘æ§ GPU
watch -n 1 nvidia-smi

# ä½¿ç”¨ Python profiler
python -m cProfile -o profile.stats scripts/vllm_rollout.py ...
python -m pstats profile.stats
```

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## è®¸å¯è¯

Apache License 2.0



