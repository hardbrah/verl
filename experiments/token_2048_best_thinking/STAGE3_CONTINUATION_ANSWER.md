# Stage3 ç»­å†™æç¤ºè¯æ„é€ æŒ‡å—

## ğŸ“‹ é—®é¢˜å›ç­”

**é—®é¢˜**ï¼šè¿™äº›éƒ½æ˜¯æ¨¡å‹è¢«ä¸­æ–­çš„æ€è€ƒè¿‡ç¨‹ï¼Œæˆ‘æƒ³è¦æ¨¡å‹æ¥ç€è¿™ä¸ªæ€ç»´è¿‡ç¨‹æ€è€ƒï¼Œè¯¥æ€æ ·æ„é€ æç¤ºè¯ï¼Ÿç›´æ¥ `tokenizer.apply_chat_template` å°±è¡Œäº†å—ï¼Ÿ

**å›ç­”**ï¼šå¯ä»¥ç”¨ `tokenizer.apply_chat_template`ï¼Œä½†è¦æ³¨æ„æ­£ç¡®çš„ç”¨æ³•ï¼

---

## âœ… æ­£ç¡®ç­”æ¡ˆ

### ä½ çš„æ•°æ®æ ¼å¼

ä½ çš„ `stage3_temp_input.parquet` ä¸­ï¼Œ`question` åˆ—å·²ç»æ˜¯**å®Œæ•´çš„å¯¹è¯åˆ—è¡¨æ ¼å¼**ï¼š

```python
[
  {
    "role": "user",
    "content": "Solve the following math problem..."
  },
  {
    "role": "assistant",
    "content": "We are given that... [æœªå®Œæˆçš„æ¨ç†è¿‡ç¨‹]"
  }
]
```

### æ­£ç¡®çš„å¤„ç†æ–¹æ³•

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# âœ… æ­£ç¡®ï¼šç›´æ¥ä¼ å…¥å¯¹è¯åˆ—è¡¨
for conversation in df['question']:
    formatted_text = tokenizer.apply_chat_template(
        conversation,              # ç›´æ¥ä¼ å…¥ï¼Œä¸è¦å†åŒ…è£…ï¼
        tokenize=False,
        add_generation_prompt=True  # å…³é”®å‚æ•°ï¼
    )
```

### å…³é”®ç‚¹è¯´æ˜

1. **ä½ çš„æ•°æ®å·²ç»æ˜¯å¯¹è¯æ ¼å¼**
   - ä¸éœ€è¦å†æ„é€  `[{"role": "user", "content": ...}]`
   - ç›´æ¥ä¼ å…¥æ•´ä¸ª `conversation` åˆ—è¡¨

2. **`add_generation_prompt=True` æ˜¯å¿…é¡»çš„**
   - è¿™ä¸ªå‚æ•°å‘Šè¯‰ tokenizer è¦æ·»åŠ ç»§ç»­ç”Ÿæˆçš„æç¤ºç¬¦
   - å¯¹äº Qwen æ¨¡å‹ï¼Œä¼šæ·»åŠ  `<|im_start|>assistant` ä½†ä¸æ·»åŠ  `<|im_end|>`
   - æ¨¡å‹ä¼šè‡ªç„¶åœ°ä»æœ€åçš„ assistant å›ç­”ç»§ç»­ç”Ÿæˆ

3. **æ•ˆæœ**
   - æ¨¡å‹ä¼šçœ‹åˆ°å®Œæ•´çš„å¯¹è¯å†å²
   - æ¨¡å‹ä¼šä»æœ€åçš„ assistant å›ç­”å¤„ç»§ç»­æ€è€ƒ
   - å°±åƒäººç±»æ¥ç€ä¹‹å‰çš„æ€è·¯ç»§ç»­æ¨ç†

---

## âŒ å¸¸è§é”™è¯¯

### é”™è¯¯ 1ï¼šé‡å¤åŒ…è£…

```python
# âŒ é”™è¯¯ï¼ä¼šå¯¼è‡´åµŒå¥—
formatted_text = tokenizer.apply_chat_template(
    [{"role": "user", "content": conversation}],  # conversation å·²ç»æ˜¯åˆ—è¡¨äº†ï¼
    tokenize=False,
    add_generation_prompt=True
)
```

**é—®é¢˜**ï¼š`conversation` æœ¬èº«å°±æ˜¯ä¸€ä¸ªå¯¹è¯åˆ—è¡¨ï¼Œå†åŒ…è£…ä¸€å±‚ä¼šå¯¼è‡´æ ¼å¼é”™è¯¯ã€‚

### é”™è¯¯ 2ï¼šå¿˜è®° `add_generation_prompt=True`

```python
# âŒ é”™è¯¯ï¼æ¨¡å‹ä¸çŸ¥é“è¦ç»§ç»­ç”Ÿæˆ
formatted_text = tokenizer.apply_chat_template(
    conversation,
    tokenize=False,
    add_generation_prompt=False  # æˆ–è€…ä¸è®¾ç½®ï¼ˆé»˜è®¤ Falseï¼‰
)
```

**é—®é¢˜**ï¼šæ²¡æœ‰ç”Ÿæˆæç¤ºç¬¦ï¼Œæ¨¡å‹ä¼šè®¤ä¸ºå¯¹è¯å·²ç»ç»“æŸï¼Œä¸ä¼šç»§ç»­ç”Ÿæˆã€‚

### é”™è¯¯ 3ï¼šåªä¼ å…¥ assistant çš„å†…å®¹

```python
# âŒ é”™è¯¯ï¼ä¸¢å¤±äº†ç”¨æˆ·é—®é¢˜
last_response = conversation[-1]['content']
formatted_text = tokenizer.apply_chat_template(
    [{"role": "assistant", "content": last_response}],
    tokenize=False,
    add_generation_prompt=True
)
```

**é—®é¢˜**ï¼šæ¨¡å‹éœ€è¦çœ‹åˆ°å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬ç”¨æˆ·çš„é—®é¢˜ã€‚

---

## ğŸ”§ éªŒè¯ä½ çš„ä»£ç 

è¿è¡ŒéªŒè¯è„šæœ¬æ£€æŸ¥æ ¼å¼ï¼š

```bash
python scripts/check_continuation_format.py
```

ä½ åº”è¯¥çœ‹åˆ°ï¼š
- âœ… æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼
- âœ… åŒ…å« 2 è½®å¯¹è¯ï¼ˆuser + assistantï¼‰
- âœ… æœ€åä¸€è½®æ˜¯ assistantï¼ˆæœªå®Œæˆï¼‰

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹ä»£ç 

```python
#!/usr/bin/env python3
import pandas as pd
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# 1. è¯»å–æ•°æ®
df = pd.read_parquet('outputs/stage3_temp_input.parquet')

# 2. åŠ è½½ tokenizer
model_path = "/datacenter/models/Qwen/Qwen3-4B-Instruct-2507"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# 3. æ ¼å¼åŒ–æç¤ºè¯
formatted_prompts = []
for conversation in df['question']:
    # conversation æ˜¯å®Œæ•´çš„å¯¹è¯åˆ—è¡¨ï¼š
    # [
    #   {"role": "user", "content": "..."},
    #   {"role": "assistant", "content": "æœªå®Œæˆçš„å›ç­”..."}
    # ]
    
    # ç›´æ¥ä¼ å…¥ï¼Œä¸è¦å†åŒ…è£…
    text = tokenizer.apply_chat_template(
        conversation,              # ç›´æ¥ä½¿ç”¨æ•´ä¸ªå¯¹è¯
        tokenize=False,
        add_generation_prompt=True  # æ·»åŠ ç»§ç»­ç”Ÿæˆçš„æç¤º
    )
    formatted_prompts.append(text)

# 4. ä½¿ç”¨ vLLM ç”Ÿæˆ
llm = LLM(
    model=model_path,
    trust_remote_code=True,
    tensor_parallel_size=1,
    max_model_len=32768
)

sampling_params = SamplingParams(
    n=8,                # æ¯ä¸ªé—®é¢˜ç”Ÿæˆ 8 æ¬¡
    temperature=1.0,
    top_p=0.95,
    max_tokens=2048
)

# 5. æ‰§è¡Œç”Ÿæˆ
outputs = llm.generate(formatted_prompts, sampling_params)

# 6. æå–ç»“æœ
for idx, output in enumerate(outputs):
    continuations = [o.text for o in output.outputs]
    print(f"æ ·æœ¬ {idx}: ç”Ÿæˆäº† {len(continuations)} ä¸ªç»­å†™")
```

---

## ğŸ¯ æ ¸å¿ƒè¦ç‚¹æ€»ç»“

| é—®é¢˜ | ç­”æ¡ˆ |
|------|------|
| éœ€è¦é‡æ–°æ„é€ å¯¹è¯æ ¼å¼å—ï¼Ÿ | âŒ ä¸éœ€è¦ï¼Œæ•°æ®å·²ç»æ˜¯å¯¹è¯æ ¼å¼ |
| å¯ä»¥ç›´æ¥ç”¨ `apply_chat_template` å—ï¼Ÿ | âœ… å¯ä»¥ï¼Œç›´æ¥ä¼ å…¥å¯¹è¯åˆ—è¡¨ |
| éœ€è¦ `add_generation_prompt=True` å—ï¼Ÿ | âœ… å¿…é¡»ï¼å¦åˆ™æ¨¡å‹ä¸ä¼šç»§ç»­ç”Ÿæˆ |
| æ˜¯å¦éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Ÿ | âŒ ä¸éœ€è¦ï¼Œæ ‡å‡†æµç¨‹å³å¯ |

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

- **éªŒè¯è„šæœ¬**: `scripts/check_continuation_format.py`
- **ç”Ÿæˆè„šæœ¬**: `scripts/simple_generate.py` (å·²æ›´æ–° `generate_responses_stage3`)
- **è¯¦ç»†æŒ‡å—**: `CONTINUATION_GUIDE.md`

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. éªŒè¯æ•°æ®æ ¼å¼
python scripts/check_continuation_format.py

# 2. è¿è¡Œç”Ÿæˆï¼ˆä½¿ç”¨æ›´æ–°åçš„ stage3 å‡½æ•°ï¼‰
python scripts/simple_generate.py \
    --model_path /datacenter/models/Qwen/Qwen3-4B-Instruct-2507 \
    --input outputs/stage3_temp_input.parquet \
    --output outputs/stage3_output.parquet \
    --n_samples 8 \
    --max_new_tokens 2048
```

æå®šï¼ğŸ‰

